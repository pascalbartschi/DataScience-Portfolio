{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2459927d",
   "metadata": {},
   "source": [
    "# Exercise 11\n",
    "Deadline: Tuesday, May 17 (end of day) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802d6fa",
   "metadata": {},
   "source": [
    "##### MapReduce\n",
    "MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. \n",
    "A MapReduce program is composed of a *map* procedure, which performs filtering and sorting, and a *reduce* method, which performs a summary operation.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*DOdiH_em5zWcZivW7ZCYxw.png \"mapreduce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d57c7",
   "metadata": {},
   "source": [
    "#### A basic MapReduce example:\n",
    "Suppose we have a list of strings and want to identify the longest string. The following python function returns the longest string by simply iterating over the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5ddc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:17:36.120933900Z",
     "start_time": "2023-08-17T12:17:36.120084500Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_longest_string(list_of_strings):\n",
    "    longest_string = None\n",
    "    longest_string_len = 0 \n",
    "    for s in list_of_strings:\n",
    "        if len(s) > longest_string_len:\n",
    "            longest_string_len = len(s)\n",
    "            longest_string = s\n",
    "    return longest_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97241caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:17:37.082148200Z",
     "start_time": "2023-08-17T12:17:37.070165400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "strings = ['find', 'longest', 'word', 'in', 'list']\n",
    "%time print(find_longest_string(strings)) # the magic command prints the time it takes to run the statement\n",
    "# why doesn't it print the cpu times of the func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c005dd9",
   "metadata": {},
   "source": [
    "What happens if we feed a list of 50 million strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfeec18c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:19:28.654323600Z",
     "start_time": "2023-08-17T12:19:27.251040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "strings = ['find', 'longest', 'word', 'in', 'list'] * 10**7\n",
    "%time print(find_longest_string(strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621e9c2",
   "metadata": {},
   "source": [
    "This is a problem, as the computation takes too long. We can break our code into smaller components and try to execute them in parallel. The idea is the following:\n",
    "1. break data into many chunks\n",
    "2. execute our function on every chunk in parallel\n",
    "3. find the longest string among the outputs of all chunks  \n",
    "\n",
    "More specifically, we compute the `len` of the string and compare it to the longest string to date, and then select the `max`value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30832558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:19:42.099219200Z",
     "start_time": "2023-08-17T12:19:38.181775100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('longest', 7)\n",
      "CPU times: total: 1.34 s\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prints the time it takes to run the cell\n",
    "\n",
    "# step 1:\n",
    "list_of_lengths = [len(s) for s in strings]\n",
    "list_of_lengths = zip(strings, list_of_lengths)\n",
    "\n",
    "# step 2:\n",
    "max_length = max(list_of_lengths, key=lambda x: x[1]) # function tells max to only look at value of index 1 , anonymou\n",
    "print(max_length)\n",
    "# why does it take even longer than above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54402d7",
   "metadata": {},
   "source": [
    "We can redefine this concept by introducing a *mapper* and a *reducer*. The mapper is just the `len` function, whereas the reducer gets two tuples and return the one with the biggest length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10076ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:19:54.489714400Z",
     "start_time": "2023-08-17T12:19:54.489208900Z"
    }
   },
   "outputs": [],
   "source": [
    "mapper = len\n",
    "\n",
    "def reducer(p, c):\n",
    "    return p if p[1] > c[1] else c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10077855",
   "metadata": {},
   "source": [
    "##### Short Theory \n",
    "\n",
    "Mapping: consists of applying a transformation function to an iterable to produce a new iterable. Items in the new iterable are produced by calling the transformation function on each item in the original iterable.\n",
    "\n",
    "Filtering: consists of applying a predicate or Boolean-valued function to an iterable to generate a new iterable. Items in the new iterable are produced by filtering out any items in the original iterable that make the predicate function return false.\n",
    "\n",
    "Reducing: consists of applying a reduction function to an iterable to produce a single cumulative value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de5bf6",
   "metadata": {},
   "source": [
    "We can now rewrite our code in the MapReduce concept using the built-in python functions `map` and `reduce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3ffa35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:20:17.949726400Z",
     "start_time": "2023-08-17T12:20:14.164149400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('longest', 7)\n",
      "CPU times: total: 1.8 s\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import reduce\n",
    "\n",
    "# step 1:\n",
    "\n",
    "# each string is 1:1 mapped to its length via the map function:\n",
    "# it returns a list of values generated using the mapper function on the elements of the strings list\n",
    "mapped = map(mapper, strings)\n",
    "\n",
    "# run this to take a look at the list of values returned by the map operation above:\n",
    "# just print few values (here the first 5)...\n",
    "# print(list(mapped)[:5])\n",
    "\n",
    "# associate the strings to their length:\n",
    "mapped = zip(strings, mapped)\n",
    "\n",
    "# print(list(mapped)[:5]) \n",
    "\n",
    "# step 2:\n",
    "# we get the pair (string, length) where length is maximum:\n",
    "# the reduced function is cumulatively applied to the mapped data\n",
    "reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4bc7e",
   "metadata": {},
   "source": [
    "At this time the code does exactly the same thing as before (hence the equal timing) since we have not parallelized anything yet. Let's split our `strings` into chunks of equal size. We use a helper function to create the data chunks. Once we have this we iterate over the chunks and store the largest word, and finally reduce to find the longest word in the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e958a8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:20:19.840302100Z",
     "start_time": "2023-08-17T12:20:19.839752100Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitter(a, n_chunks):\n",
    "    '''\n",
    "    Splits the list \"a\" in \"n_chunks\" chunks.\n",
    "    '''\n",
    "    k, m = divmod(len(a), n_chunks)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d8e80",
   "metadata": {},
   "source": [
    "#### What does divmod() do?\n",
    "\n",
    "For integers, the return value is the same as (a // b, a % b).\n",
    "\n",
    "For floating point numbers the return value is (q, a % b), where q is usually math.floor(a / b) which is the whole part of the quotient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfb5281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:20:24.769401400Z",
     "start_time": "2023-08-17T12:20:24.666776800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 50 chunks has 1000000 strings.\n"
     ]
    }
   ],
   "source": [
    "data_chunks = splitter(strings, 50)\n",
    "print(f'Each of the {len(data_chunks)} chunks has {len(data_chunks[0])} strings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318fda01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:20:29.816196400Z",
     "start_time": "2023-08-17T12:20:25.837724600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('longest', 7)\n",
      "CPU times: total: 1.64 s\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# step 1:\n",
    "reduced_all = []\n",
    "\n",
    "# for each chunk of data we perform the map and reduce operations\n",
    "for chunk in data_chunks:\n",
    "    mapped_chunk = map(mapper, chunk)\n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    \n",
    "    reduced_chunk = reduce(reducer, mapped_chunk)\n",
    "    # store the result of the reduce operation \n",
    "    reduced_all.append(reduced_chunk)\n",
    "\n",
    "# step 2:\n",
    "# reduce over all the chunks\n",
    "global_reduced = reduce(reducer, reduced_all)\n",
    "print(global_reduced)\n",
    "\n",
    "# why doesn't the time really decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8ac43",
   "metadata": {},
   "source": [
    "We can unify the `map` and `reduce` in the for loop in a single function. This will help us when we run the code in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1b10eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:20:37.020939Z",
     "start_time": "2023-08-17T12:20:33.117802300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('longest', 7)\n",
      "CPU times: total: 1.36 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def chunks_mapper(chunk):\n",
    "    mapped_chunk = map(mapper, chunk)\n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    return reduce(reducer, mapped_chunk)\n",
    "\n",
    "# step 1:\n",
    "mapped = map(chunks_mapper, data_chunks)  # for each of the data chunnks the chunks mapper is applied\n",
    "\n",
    "# step2:\n",
    "global_reduced = reduce(reducer, mapped)\n",
    "print(global_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4476ee4",
   "metadata": {},
   "source": [
    "The final ingredient is to parallelize the `for` loop (step 1) using the `multiprocessing` module by using the `pool.map` instead of our regular `map` function.\n",
    "***NOTE: this part might not work on Mac/Windows with Python >3.8!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059fae2",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-17T12:20:47.727403100Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# create a Pool object\n",
    "# the processes parameter defines how many processes (threads) to split the job into\n",
    "pool = Pool(processes=32)\n",
    "\n",
    "# we want a separate process for each chunk of data\n",
    "# processes == number of chunks\n",
    "data_chunks = splitter(strings, 32)\n",
    "\n",
    "mapper = len\n",
    "# step 1: now runs in parallel --> use pool.map instead of just map\n",
    "mapped = pool.map(chunks_mapper, data_chunks)\n",
    "\n",
    "# step 2:\n",
    "global_reduced = reduce(reducer, mapped)\n",
    "pool.close()\n",
    "print(global_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c6e18",
   "metadata": {},
   "source": [
    "---\n",
    "Suppose we have a very large collection of articles or twitter tweets with many words and we would like to find the top 10 used words in the entire collection. We are going to use MapReduce on a built-in dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63613523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T12:25:41.948194600Z",
     "start_time": "2023-08-17T12:23:53.633402700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups()\n",
    "\n",
    "extend = 1\n",
    "data = news.data * extend\n",
    "print('Number of articles:', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25a273",
   "metadata": {},
   "source": [
    "We need to do some preliminary cleaning of the dataset: clean words, remove stop words and non-english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5c627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Â here you might want to import stop_words instead of _stop_words\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^\\w\\s]','',word).lower()\n",
    "\n",
    "def word_not_in_stopwords(word):\n",
    "    return word not in _stop_words.ENGLISH_STOP_WORDS and word and word.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a496556",
   "metadata": {},
   "source": [
    "The following function finds the top 10 words in the entire collection. We use the `Counter` object to keep track of individual words and counts by using its `update` routine. Individual tokens in the text have to be cleaned and filtered for `ENGLISH_STOP_WORDS`. Use the two functions given above together with the built-in `filter` routine to clean the tokens. We fill in the function below and find the 10 most common words by running it.\n",
    "\n",
    "* We fill in the function below: map every word in the text to their 'cleaned' version obtained through the function `clean_word`. Then, use the `find_top_words` function to compute the 10 most common word in the articles in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aeff6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_top_words(data):\n",
    "    cnt = Counter() # the counter object\n",
    "    for text in data: # loop over the articles in the dataset\n",
    "        tokens_in_text = text.split() # get the list of different words in the text\n",
    "        \n",
    "        tokens_in_text = list(map(clean_word, tokens_in_text))\n",
    "        \n",
    "        # filter the tokens for word_not_in_stopwords() using filter()\n",
    "        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "        \n",
    "        # update the counter object with the cleaned texts\n",
    "        cnt.update(tokens_in_text)\n",
    "    \n",
    "    # return the 10 most common words in the texts in the counter object\n",
    "    return cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae37a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subject', 12252),\n",
       " ('lines', 11824),\n",
       " ('organization', 11185),\n",
       " ('writes', 7836),\n",
       " ('article', 6754),\n",
       " ('people', 5832),\n",
       " ('dont', 5813),\n",
       " ('like', 5757),\n",
       " ('just', 5579),\n",
       " ('university', 5544)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_top_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1840d7a",
   "metadata": {},
   "source": [
    "* We implement the `mapper`, `reducer` and `chunk_mapper` functions for this specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45e2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this function to accept some text to:\n",
    "# - split it into words\n",
    "# - to clean the words\n",
    "# - return a counter object created out of these cleaned words (already implemented below)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def mapper(text):\n",
    "    text = text.split()\n",
    "    tokens_in_text = map(clean_word, text)\n",
    "    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "    \n",
    "    # print(Counter(tokens_in_text))\n",
    "    return Counter(tokens_in_text)\n",
    "\n",
    "# You want this function to update the counter object cnt1 with a second counter object cnt2\n",
    "def reducer(cnt1, cnt2):\n",
    "    cnt1.update(cnt2)\n",
    "    return cnt1\n",
    "\n",
    "# It has to map the data in the chunk with the mapper function and reduce them\n",
    "# (the reduce operation consists in the cumulative update performed by the reducer)\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(mapper, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f383dd",
   "metadata": {},
   "source": [
    "* We split the dataset into 16 individual chunks though the `splitter` function (see above) and initiate a `Pool` object with 1 process per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd018b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "data_chunks = splitter(data, 16)\n",
    "\n",
    "# pool = #  Pool(processes = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8225bc",
   "metadata": {},
   "source": [
    "* We implement the MapReduce scheme (see the example above) and run the code in parallel using the functions you have just defined. Display the 5 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5943b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('subject', 12252) ('lines', 11824) ('organization', 11185) ('writes', 7836) ('article', 6754)\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# step 1:\n",
    "# mapped = pool.map(chunk_mapper, data_chunks)\n",
    "\n",
    "mapped = map(chunk_mapper, data_chunks)\n",
    "\n",
    "# step 2:\n",
    "# reduced = reduce(reducer, mapped)\n",
    "global_reduced = reduce(reducer, mapped)\n",
    "\n",
    "# print global\n",
    "\n",
    "print(*global_reduced.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e522d50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We change the `extend` keyword in the cell where we load the data from 1 to 5. What do you observe in the timings of the parallelized and non-parallelized cases. How do both the timings of the parallelized and non-parallelized cases scale with the `extend` factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a23110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timings scaling with the extend factor\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups()\n",
    "\n",
    "extend = 5\n",
    "data = news.data * extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7efcf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subject', 61260), ('lines', 59120), ('organization', 55925), ('writes', 39180), ('article', 33770)]\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# non-parallized approach\n",
    "\n",
    "mapped = map(mapper, data)\n",
    "\n",
    "reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(reduced.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1922e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subject', 61260), ('lines', 59120), ('organization', 55925), ('writes', 39180), ('article', 33770)]\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parallized approach\n",
    "\n",
    "# generate chunks\n",
    "\n",
    "data_chunks = splitter(data, 16 * extend)\n",
    "\n",
    "mapped = map(chunk_mapper, data_chunks)\n",
    "\n",
    "# step 2:\n",
    "# reduced = reduce(reducer, mapped)\n",
    "global_reduced = reduce(reducer, mapped)\n",
    "\n",
    "# print global\n",
    "\n",
    "print(global_reduced.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629097f",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "By incresing the data by a factor of 5 the calculation time to generate the reduced counter object increases by a factor of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab0278",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using the MapReduce paradigm, we find the word with the largest number of consonants in the text you find in the `some.txt` file. Contracted forms like \"I'm\" or \"don't\", and genitive forms like \"hunter's\" shall be considered as different words with respect to their non-contracted, or non genitive, versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b9c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the word with the largest number of consonants in the text\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "raw  = open('data/some.txt')\n",
    "some = raw.readlines()[0]\n",
    "raw.close()\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "# define functions that clean strings, count consonants and reduce mapping\n",
    "\n",
    "\n",
    "def string_cleaner(string):\n",
    "    trash = \"()?'-,\"\n",
    "    for t in trash:\n",
    "        string = string.replace(t, \" \")\n",
    "\n",
    "    string = string.replace('  ',' ')\n",
    "    \n",
    "    return string.split() \n",
    "\n",
    "def cons_counter(word):\n",
    "                                      \n",
    "    letters = Counter(word)\n",
    "    del letters['a'], letters['e'], letters['i'],letters['o'],letters['u']\n",
    "    \n",
    "    return (sum(letters.values()), word)\n",
    "\n",
    "def reducer(word1, word2):\n",
    "    return word1 if word1[0] > word2[0] else word2\n",
    "\n",
    "# implementation of optimized parallel mapping\n",
    "\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(cons_counter, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10a98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word with the most consonants is `Searchlights` which contains 9 consonants\n"
     ]
    }
   ],
   "source": [
    "# call the functions\n",
    "\n",
    "some = string_cleaner(some)\n",
    "chunks = splitter(some, 30)\n",
    "\n",
    "# cons_counter('elsa')\n",
    "\n",
    "mapped = map(chunk_mapper, chunks)\n",
    "\n",
    "global_reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(('The word with the most consonants is `{}` which contains {} consonants').format(global_reduced[1], global_reduced[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
