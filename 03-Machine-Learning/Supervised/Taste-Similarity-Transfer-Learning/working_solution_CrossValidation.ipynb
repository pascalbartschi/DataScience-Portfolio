{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16163d2dd773fbc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Task 3\n",
    "This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n",
    "This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1a3a9db8e3f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "824a840beb8b323e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "# data handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82adb41ca8c23be6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The device is automatically set to GPU if available, otherwise CPU\n",
    "# If you want to force the device to CPU, you can change the line to\n",
    "# device = torch.device(\"cpu\")\n",
    "# When using the GPU, it is important that your model and all data are on the \n",
    "# same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c4e1470-4d4d-49d3-915d-80ece46ff2ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "the embeddings.\n",
    "\"\"\"\n",
    "if not os.path.exists(\"dataset/embeddings.npy\"):\n",
    "    # TODO: define a transform to pre-process the images\n",
    "    # The required pre-processing depends on the pre-trained model you choose \n",
    "    # below. \n",
    "    # See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n",
    "    data_transforms = {\n",
    "        \"Advanced\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"Alternative\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize([0.6110, 0.5012, 0.3752], [0.2575, 0.2659, 0.2801])\n",
    "        ]), \n",
    "        \"Simple\": transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=data_transforms[\"Advanced\"])\n",
    "    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "    # run out of memory (VRAM if on GPU, RAM if on CPU)\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True, \n",
    "                            num_workers=0)\n",
    "\n",
    "    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model, \n",
    "    # more info here: https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "    model = resnet50(weights=\"DEFAULT\") # other weights to try: \n",
    "    # The dimensionality of a vector embedding is equivalent to the size of the second-to-last layer in the model and, \n",
    "    # thus, interchangeable with the vectorâ€™s size or length.\n",
    "    embedding_size = model.fc.in_features\n",
    "    num_images = len(train_dataset)\n",
    "\n",
    "    # Remove the last layer (fully connected layer)\n",
    "    model = nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "    # Freeze the parameters of the model\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Extract embeddings\n",
    "    with torch.no_grad():\n",
    "        for img, _ in train_loader:\n",
    "            img = img.to(device)\n",
    "            output = model(img)\n",
    "            output = output.view(output.size(0), -1)  # Flatten the output\n",
    "            embeddings.append(output.cpu().numpy())\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "    # model to access the embeddings the model generates. \n",
    "\n",
    "    assert embeddings.shape == (num_images, embedding_size)\n",
    "\n",
    "    np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "335d91cc379d4f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line.strip())\n",
    "\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                        transform=None)\n",
    "    filenames = [s[0].split('/')[-1].split(\"\\\\\")[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    # Normalize the embeddings\n",
    "    embeddings = (embeddings - embeddings.mean(axis = 1)[:, np.newaxis]) / embeddings.std(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48f07a1c0c478",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory (VRAM if on GPU, RAM if on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6daf836a4adb0abe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y = None, train = True, batch_size=50, shuffle=True, num_workers = 4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1baa5918f11a049",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "TODO: define a model. Here, the basic structure is defined, but you need to fill in the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcd11318eb7b9488",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BinaryTasteClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier with 4 fully connected layers, activated by ReLu and including dropout\n",
    "    for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, dropout_proba):\n",
    "        #inherit from Module\n",
    "        super().__init__()\n",
    "\n",
    "        # specify layer objects\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_proba)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_proba)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size3)\n",
    "        self.dropout3 = nn.Dropout(dropout_proba)\n",
    "        self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "\n",
    "        # initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # layer1\n",
    "        x = self.fc1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout1(x)\n",
    "        \n",
    "        # layer2\n",
    "        x = self.fc2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout2(x)\n",
    "        \n",
    "        # layer3\n",
    "        x = self.fc3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.dropout3(x)\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48607b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpl loader created\n",
      "train loader created\n",
      "valid loader created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "\n",
    "# load the training data\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "\n",
    "# simple train loader\n",
    "train_loader_simple = create_loader_from_np(X, y, train = True, batch_size=64)\n",
    "print(\"simpl loader created\")\n",
    "# create KFold object\n",
    "n_splits = 10\n",
    "kfold = KFold(n_splits=n_splits, shuffle = True, random_state=42)\n",
    "# Create data loaders for the training data\n",
    "train_loader = {fold: create_loader_from_np(X = X[train_indices], y = y[train_indices], train = True, batch_size=64)\n",
    "                       for fold, (train_indices, _) in enumerate(kfold.split(X))}\n",
    "print(\"train loader created\")\n",
    "val_loader = {fold: create_loader_from_np(X = X[val_indices], y = y[val_indices], train = True, batch_size=64)\n",
    "                       for fold, (_, val_indices) in enumerate(kfold.split(X))}\n",
    "print(\"valid loader created\")\n",
    "# create the data loader\n",
    "data_loader = {\"Train\": train_loader,\n",
    "               \"Valid\": val_loader}\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "# delete the loaded training data to save memory, as the data loader copies\n",
    "del X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68d41036",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "# repeat for testing data\n",
    "X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d35b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model:nn.Sequential, \n",
    "                n_epochs:int,\n",
    "                criterion:nn,\n",
    "                optimizer:torch.optim,\n",
    "                scheduler:lr_scheduler,\n",
    "                data_loader:DataLoader,\n",
    "                prediction_threshold:float = 0.5):\n",
    "    \"\"\"\n",
    "    This f\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): CNN model class\n",
    "        n_epochs (int): number of epochs to train\n",
    "        criterion (nn): loss function\n",
    "        optimizer (nn.optim): optimization  algorithm\n",
    "        scheduler (lr_scheduler): learning rate adaption over epochs\n",
    "        data_loader (DataLoader): data loader object holding training and validation data\n",
    "        prediction_threshold (float, optional): decision boundary for binary classification. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: best model weigths found\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        # info\n",
    "        print(53 * \"#\")\n",
    "        print(20 * \"~\", f' EPOCH {epoch + 1}/{n_epochs}', 20 * \"~\")\n",
    "        print(53 * \"#\")\n",
    "        # start time of epoch\n",
    "        epoch_since = time.time()\n",
    "        # decide which fold to use\n",
    "        if epoch % n_epochs == 0: fold = 0\n",
    "\n",
    "        # train and validation phase looping   \n",
    "        for train in [True, False]:\n",
    "            # set phase string\n",
    "            phase = \"Train\" if train else \"Valid\"\n",
    "\n",
    "            # set model mode\n",
    "            if train: \n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            # init data loader\n",
    "            data_loader_phase = copy.deepcopy(data_loader[phase][fold])\n",
    "\n",
    "            # init epoch loss and corrects\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_labels = 0\n",
    "            i = 1\n",
    "\n",
    "            with tqdm(data_loader_phase, unit = \"batch\") as tepoch:\n",
    "            # iterate data loader\n",
    "                for inputs, labels in tepoch: # tepoch: \n",
    "                    # set progressbar description and postfix\n",
    "                    tepoch.set_description(f\"{phase} phase\")\n",
    "                    tepoch.set_postfix({\"LOSS\": running_loss/i, \"ACC\": running_corrects/ max(total_labels)})\n",
    "                    # copy to device\n",
    "                    inputs.to(device)\n",
    "                    labels.to(device)\n",
    "\n",
    "                    # set the gradient to zero\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # step forward, track only if in train\n",
    "                    with torch.set_grad_enabled(train):\n",
    "                        outputs = model(inputs).squeeze(1)# .round().to(torch.long) # removing singleton dimension at axis 1\n",
    "                        preds = np.where(F.sigmoid(outputs).detach().numpy() >= prediction_threshold, 1, 0)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # propagate backwards if in train phase\n",
    "                        if train:\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                \n",
    "\n",
    "                    # calculate stats\n",
    "                    running_loss += loss.item()\n",
    "                    running_corrects += np.sum(preds == labels.numpy()).item()\n",
    "                    total_labels += len(labels)\n",
    "                    i += 1\n",
    "\n",
    "                if train: \n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / i\n",
    "                epoch_accuracy = running_corrects / max(1, total_labels)\n",
    "                # epoch_time = time.time() - epoch_since\n",
    "\n",
    "                # print(f'Final {phase}: Loss = {epoch_loss:.4f}, Acc = {epoch_accuracy:.4f}, Time = {epoch_time:.0f}s')\n",
    "\n",
    "                if not train and epoch_accuracy > best_accuracy:\n",
    "                        best_accuracy = epoch_accuracy\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best Valid Acc: {best_accuracy:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return (best_accuracy, best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bc51b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 1/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [01:14<00:00, 22.48batch/s, LOSS=129, ACC=0.808]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:06<00:00, 30.98batch/s, LOSS=127, ACC=0.729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 2/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:54<00:00, 30.59batch/s, LOSS=126, ACC=0.889]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 35.34batch/s, LOSS=125, ACC=0.791] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 3/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:53<00:00, 31.24batch/s, LOSS=121, ACC=0.963]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 34.11batch/s, LOSS=119, ACC=0.851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 4/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:53<00:00, 31.31batch/s, LOSS=119, ACC=0.974]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:04<00:00, 37.81batch/s, LOSS=118, ACC=0.856] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 5/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:50<00:00, 32.88batch/s, LOSS=117, ACC=0.971]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:04<00:00, 37.28batch/s, LOSS=116, ACC=0.849] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 6/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:52<00:00, 31.82batch/s, LOSS=117, ACC=0.965]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 35.70batch/s, LOSS=116, ACC=0.845] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 7/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:52<00:00, 31.92batch/s, LOSS=117, ACC=0.966]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:04<00:00, 37.33batch/s, LOSS=115, ACC=0.845] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 8/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:51<00:00, 32.62batch/s, LOSS=116, ACC=0.965]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 36.89batch/s, LOSS=117, ACC=0.847] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 9/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [00:51<00:00, 32.22batch/s, LOSS=117, ACC=0.964]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 31.30batch/s, LOSS=116, ACC=0.846] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 10/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "#####################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1674/1674 [01:08<00:00, 24.27batch/s, LOSS=117, ACC=0.964]\n",
      "Valid phase: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:05<00:00, 31.23batch/s, LOSS=115, ACC=0.852]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 55m 5s\n",
      "Best Valid Acc: 0.855785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training procedure of the model; it accepts the training data, defines the model \n",
    "and then trains it.\n",
    "\n",
    "input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "compute: model: torch.nn.Module, the trained model\n",
    "\"\"\"\n",
    "\n",
    "model = BinaryTasteClassifier(input_size=6144, \n",
    "                              hidden_size1=1024, \n",
    "                              hidden_size2=64,\n",
    "                              hidden_size3=32,\n",
    "                              dropout_proba=0.5)\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 10\n",
    "# TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "# of the training data as a validation split. After each epoch, compute the loss on the \n",
    "# validation split and print it out. This enables you to see how your model is performing \n",
    "# on the validation data before submitting the results on the server. After choosing the \n",
    "# best model, train it on the whole training data.\n",
    "\n",
    "# define the loss criterion\n",
    "criterion_CE = nn.CrossEntropyLoss()\n",
    "criterion_MSE = nn.MSELoss()\n",
    "\n",
    "# define possible optimizers\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.3)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler to adjust learning rate over epoch iterations\n",
    "sgd_lr_scheduler = lr_scheduler.StepLR(optimizer_sgd, step_size=2, gamma=0.1)\n",
    "adam_lr_scheduler = lr_scheduler.StepLR(optimizer_adam, step_size=7, gamma=0.1)\n",
    "\n",
    "FineTuningMode = True\n",
    "\n",
    "if FineTuningMode:\n",
    "    _ = train_model(model=model,\n",
    "                    n_epochs=10,\n",
    "                    criterion=criterion_CE, \n",
    "                    optimizer=optimizer_sgd, \n",
    "                    scheduler=sgd_lr_scheduler,\n",
    "                    data_loader=data_loader,\n",
    "                    prediction_threshold=0.5\n",
    "                    )\n",
    "    # MSE with sgd: 0.717851\n",
    "    # CE with sgd: \n",
    "    # MSE with adam: \n",
    "    # MSE with adam: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75d8e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1/6\n",
      "Loss: 130.181, Acc:0.636\n",
      "Epoch=2/6\n",
      "Loss: 127.722, Acc:0.721\n",
      "Epoch=3/6\n",
      "Loss: 126.308, Acc:0.750\n",
      "Epoch=4/6\n",
      "Loss: 126.047, Acc:0.757\n",
      "Epoch=5/6\n",
      "Loss: 125.833, Acc:0.762\n",
      "Epoch=6/6\n",
      "Loss: 125.817, Acc:0.763\n"
     ]
    }
   ],
   "source": [
    "# finally train the model on all the data without validation\n",
    "model = BinaryTasteClassifier(input_size=6144, \n",
    "                              hidden_size1=1024, \n",
    "                              hidden_size2=64,\n",
    "                              hidden_size3=32,\n",
    "                              dropout_proba=0.5)\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 6\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch={epoch + 1}/{n_epochs}')\n",
    "    running_loss = 0.0     \n",
    "    running_corrects = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    for i, [features, labels] in enumerate(train_loader_simple):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features).squeeze(1)\n",
    "        preds = np.where(F.sigmoid(outputs).detach().numpy() >= prediction_threshold, 1, 0)\n",
    "        running_corrects += np.sum(preds == labels.numpy()).item()\n",
    "        total_labels += len(labels)\n",
    "        # if np.sum(preds == labels.numpy()).item() > 54:\n",
    "        #     print(len(labels), len(preds))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    \n",
    "    print(f\"Loss: {running_loss / i:.3f}, Acc:{running_corrects / total_labels:.3f}\")\n",
    "    \n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2ea99b26c348253",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "then tests the model on it.\n",
    "\n",
    "input: model: torch.nn.Module, the trained model\n",
    "       loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "compute: None, the function saves the predictions to a results.txt file\n",
    "\"\"\"\n",
    "model.eval()\n",
    "predictions = []\n",
    "# Iterate over the test data\n",
    "with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "    for [x_batch] in test_loader:\n",
    "        x_batch= x_batch.to(device)\n",
    "        predicted = model(x_batch)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        # Rounding the predictions to 0 or 1\n",
    "        predicted[predicted >= 0.5] = 1\n",
    "        predicted[predicted < 0.5] = 0\n",
    "        predictions.append(predicted)\n",
    "    predictions = np.vstack(predictions)\n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
