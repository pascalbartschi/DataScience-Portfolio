{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16163d2dd773fbc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f1a3a9db8e3f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824a840beb8b323e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "# data handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82adb41ca8c23be6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The device is automatically set to GPU if available, otherwise CPU\n",
    "# If you want to force the device to CPU, you can change the line to\n",
    "# device = torch.device(\"cpu\")\n",
    "# When using the GPU, it is important that your model and all data are on the \n",
    "# same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223c393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4e1470-4d4d-49d3-915d-80ece46ff2ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File <dataset/train_embeddings.npy> saved!\n",
      "File <dataset/valid_embeddings.npy> saved!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "the embeddings.\n",
    "\"\"\"\n",
    "if not all(os.path.exists(path) for path in [\"dataset/train_embeddings.npy\", \n",
    "                                             \"dataset/valid_embeddings.npy\"]):\n",
    "\n",
    "    # See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n",
    "    \n",
    "    # data transforms as found in: https://www.analyticsvidhya.com/blog/2023/02/fast-food-classification-using-transfer-learning-with-pytorch/\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # define the model to embed into from <https://pytorch.org/vision/stable/models.html)\n",
    "    model = resnet50(weights=\"DEFAULT\") \n",
    "    # The dimensionality of a vector embedding is equivalent to the size of the second-to-last layer in the model and, \n",
    "    # thus, interchangeable with the vector’s size or length.\n",
    "    embedding_size = model.fc.in_features\n",
    "    # remove the last layer (fully connected layer)\n",
    "    model = nn.Sequential(*(list(model.children())[:-1]))\n",
    "    # rove the model to device\n",
    "    model = model.to(device)\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # freeze the parameters of the model\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "\n",
    "    # prepare embeddings for train and validation set\n",
    "    for setname in [\"train\", \"valid\"]:\n",
    "        dataset = datasets.ImageFolder(root=\"dataset/\", transform=data_transforms[setname])\n",
    "        # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "        # run out of memory (VRAM if on GPU, RAM if on CPU)\n",
    "        loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True, \n",
    "                            num_workers=0)\n",
    "\n",
    "\n",
    "        num_images = len(dataset)\n",
    "\n",
    "         # extract embeddings from second to last layer of model\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for img, _ in loader:\n",
    "                img = img.to(device)\n",
    "                output = model(img)\n",
    "                output = output.view(output.size(0), -1)  # Flatten the output\n",
    "                embeddings.append(output.cpu().numpy())\n",
    "\n",
    "        # concatenate all embeddings\n",
    "        embeddings = np.concatenate(embeddings, axis=0) \n",
    "\n",
    "        assert embeddings.shape == (num_images, embedding_size)\n",
    "\n",
    "        # save and notify\n",
    "        filename = f'dataset/{setname}_embeddings.npy'\n",
    "        np.save(filename, embeddings)\n",
    "        print(\"File <\" + filename + \"> saved!\")\n",
    "\n",
    "        # rm\n",
    "        del filename, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335d91cc379d4f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line.strip())\n",
    "\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                        transform=None)\n",
    "    filenames = [s[0].split('/')[-1].split(\"\\\\\")[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    # Normalize the embeddings\n",
    "    embeddings = (embeddings - embeddings.mean(axis = 1)[:, np.newaxis]) / embeddings.std(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc48f07a1c0c478",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory (VRAM if on GPU, RAM if on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daf836a4adb0abe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y = None, train = True, batch_size=50, shuffle=True, num_workers = 4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1baa5918f11a049",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "TODO: define a model. Here, the basic structure is defined, but you need to fill in the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd11318eb7b9488",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(6144, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class Net2(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(6144, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "# define hyperparameters\n",
    "input_size = int(6144) # 3 * embedding size (2048)\n",
    "hidden_layer = int(512)\n",
    "dropout_proba = float(0.5)\n",
    "output_size = int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48607b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "\n",
    "# load the training data\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "\n",
    "# simple train loader\n",
    "train_loader_simple = create_loader_from_np(X, y, train = True, batch_size=64)\n",
    "# create KFold object\n",
    "n_splits = 10\n",
    "kfold = KFold(n_splits=n_splits, shuffle = True, random_state=42)\n",
    "# Create data loaders for the training data\n",
    "train_loader = {fold: create_loader_from_np(X = X[train_indices], y = y[train_indices], train = True, batch_size=64)\n",
    "                       for fold, (train_indices, _) in enumerate(kfold.split(X))}\n",
    "val_loader = {fold: create_loader_from_np(X = X[val_indices], y = y[val_indices], train = True, batch_size=64)\n",
    "                       for fold, (_, val_indices) in enumerate(kfold.split(X))}\n",
    "# create the data loader\n",
    "data_loader = {\"Train\": copy.deepcopy(train_loader),\n",
    "               \"Valid\": copy.deepcopy(val_loader)}\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "# delete the loaded training data to save memory, as the data loader copies\n",
    "del X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d41036",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "# repeat for testing data\n",
    "X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n",
    "test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28634c90281cd699",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The training procedure of the model; it accepts the training data, defines the model \n",
    "and then trains it.\n",
    "\n",
    "input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "compute: model: torch.nn.Module, the trained model\n",
    "\"\"\"\n",
    "\n",
    "model = Net()\n",
    "model.train()\n",
    "model.to(device)\n",
    "n_epochs = 10\n",
    "# TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "# of the training data as a validation split. After each epoch, compute the loss on the \n",
    "# validation split and print it out. This enables you to see how your model is performing \n",
    "# on the validation data before submitting the results on the server. After choosing the \n",
    "# best model, train it on the whole training data.\n",
    "\n",
    "# define the loss criterion\n",
    "criterion_CE = nn.CrossEntropyLoss()\n",
    "criterion_MSE = nn.MSELoss()\n",
    "\n",
    "# define possible optimizers\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.0)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler to adjust learning rate over epoch iterations\n",
    "sgd_lr_scheduler = lr_scheduler.StepLR(optimizer_sgd, step_size=7, gamma=0.1)\n",
    "adam_lr_scheduler = lr_scheduler.StepLR(optimizer_adam, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.7, total_iters=n_epochs)\n",
    "# criterion = nn.MSELoss()\n",
    "n_epochs = 10\n",
    "criterion = criterion_MSE\n",
    "optimizer = optimizer_sgd\n",
    "scheduler = sgd_lr_scheduler\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "# for epoch in range(n_epochs):        \n",
    "#     print(f'\\nepoch={epoch}')\n",
    "#     running_loss = 0.0      \n",
    "#     running_corrects = 0\n",
    "\n",
    "#     for i, [X, y] in enumerate(train_loader_simple):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(X).squeeze(1)\n",
    "#         preds = np.where(F.sigmoid(output).detach().numpy() >= prediction_threshold, 1, 0)\n",
    "#         # print(F.sigmoid(output))\n",
    "#         # print((preds == y.T.numpy()))\n",
    "#         loss = criterion(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         running_corrects += np.sum(preds == y.numpy().T).item()\n",
    "#         # print(running_corrects)\n",
    "\n",
    "#         if i % 500 == 499:\n",
    "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500:.3f}')\n",
    "#             print(f'[{epoch + 1}, {i + 1:5d}] acc: {running_corrects / (500*len(y)):.3f}')\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "def train_model(model:nn.Sequential, \n",
    "                n_epochs:int,\n",
    "                criterion:nn,\n",
    "                optimizer:torch.optim,\n",
    "                scheduler:lr_scheduler,\n",
    "                data_loader:DataLoader,\n",
    "                prediction_threshold:float = 0.5):\n",
    "    \"\"\"\n",
    "    This f\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): CNN model class\n",
    "        n_epochs (int): number of epochs to train\n",
    "        criterion (nn): loss function\n",
    "        optimizer (nn.optim): optimization  algorithm\n",
    "        scheduler (lr_scheduler): learning rate adaption over epochs\n",
    "        data_loader (DataLoader): data loader object holding training and validation data\n",
    "        prediction_threshold (float, optional): decision boundary for binary classification. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: best model weigths found\n",
    "    \"\"\"\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "        # info\n",
    "        print(53 * \"#\")\n",
    "        print(20 * \"~\", f' EPOCH {epoch + 1}/{n_epochs}', 20 * \"~\")\n",
    "        print(53 * \"#\")\n",
    "        # start time of epoch\n",
    "        epoch_since = time.time()\n",
    "        # decide which fold to use\n",
    "        if epoch % n_epochs == 0: fold = 0\n",
    "\n",
    "        # train and validation phase looping   \n",
    "        for train in [True, False]:\n",
    "            # set phase string\n",
    "            phase = \"Train\" if train else \"Valid\"\n",
    "\n",
    "            # set model mode\n",
    "            if train: \n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            # init data loader\n",
    "            data_loader_phase = copy.deepcopy(data_loader[phase][fold])\n",
    "\n",
    "            # init epoch loss and corrects\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            i = 1\n",
    "\n",
    "            with tqdm(data_loader_phase, unit = \"batch\") as tepoch:\n",
    "            # iterate data loader\n",
    "                for inputs, labels in tepoch: # tepoch: \n",
    "                    # set progressbar description and postfix\n",
    "                    tepoch.set_description(f\"{phase} phase\")\n",
    "                    tepoch.set_postfix({\"LOSS\": running_loss/i, \"ACC\": running_corrects/ (i*len(labels))})\n",
    "                    # copy to device\n",
    "                    inputs.to(device)\n",
    "                    labels.to(device)\n",
    "\n",
    "                    # set the gradient to zero\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # step forward, track only if in train\n",
    "                    with torch.set_grad_enabled(train):\n",
    "                        outputs = model(inputs).squeeze(1)# .round().to(torch.long) # removing singleton dimension at axis 1\n",
    "                        preds = np.where(F.sigmoid(outputs).detach().numpy() >= prediction_threshold, 1, 0)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # propagate backwards if in train phase\n",
    "                        if train:\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                \n",
    "\n",
    "                    # calculate stats\n",
    "                    running_loss += loss.item()\n",
    "                    running_corrects += np.sum(preds == labels.numpy()).item()\n",
    "                    i += 1\n",
    "\n",
    "                if train: \n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / i\n",
    "                epoch_accuracy = running_corrects / (i*len(labels))\n",
    "                # epoch_time = time.time() - epoch_since\n",
    "\n",
    "                # print(f'Final {phase}: Loss = {epoch_loss:.4f}, Acc = {epoch_accuracy:.4f}, Time = {epoch_time:.0f}s')\n",
    "\n",
    "                if not train and epoch_accuracy > best_accuracy:\n",
    "                        best_accuracy = epoch_accuracy\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best Valid Acc: {best_accuracy:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc51b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 1/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:41<00:00, 40.83batch/s, LOSS=0.0247, ACC=0.799]\n",
      "Valid phase: 100%|██████████| 186/186 [00:06<00:00, 30.78batch/s, LOSS=0.023, ACC=0.699] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 2/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:48<00:00, 34.76batch/s, LOSS=0.0228, ACC=0.802]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 36.00batch/s, LOSS=0.0196, ACC=0.717] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 3/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:44<00:00, 37.95batch/s, LOSS=0.0209, ACC=0.805]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 36.88batch/s, LOSS=0.019, ACC=0.681] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 4/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:43<00:00, 38.53batch/s, LOSS=0.019, ACC=0.807]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 35.44batch/s, LOSS=0.0184, ACC=0.705] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 5/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:44<00:00, 37.72batch/s, LOSS=0.0187, ACC=0.808]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 37.17batch/s, LOSS=0.0191, ACC=0.695] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 6/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:52<00:00, 31.83batch/s, LOSS=0.0183, ACC=0.809]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 33.79batch/s, LOSS=0.0209, ACC=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 7/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:46<00:00, 35.83batch/s, LOSS=0.0178, ACC=0.811]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 37.15batch/s, LOSS=0.0244, ACC=0.685] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 8/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:45<00:00, 36.80batch/s, LOSS=0.0186, ACC=0.807]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 35.50batch/s, LOSS=0.0159, ACC=0.718] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 9/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:51<00:00, 32.63batch/s, LOSS=0.0184, ACC=0.808]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 35.27batch/s, LOSS=0.0161, ACC=0.712] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "~~~~~~~~~~~~~~~~~~~~  EPOCH 10/10 ~~~~~~~~~~~~~~~~~~~~\n",
      "###################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train phase: 100%|██████████| 1674/1674 [00:51<00:00, 32.65batch/s, LOSS=0.0184, ACC=0.809]\n",
      "Valid phase: 100%|██████████| 186/186 [00:05<00:00, 34.04batch/s, LOSS=0.0152, ACC=0.716] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 8m 59s\n",
      "Best Valid Acc: 0.717851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0083, -0.0088,  0.0073,  ...,  0.0126,  0.0089,  0.0012],\n",
       "                      [-0.0073, -0.0019,  0.0068,  ..., -0.0107, -0.0019, -0.0087],\n",
       "                      [-0.0121, -0.0036, -0.0009,  ..., -0.0035, -0.0114, -0.0089],\n",
       "                      ...,\n",
       "                      [-0.0021,  0.0019,  0.0097,  ..., -0.0054, -0.0083,  0.0122],\n",
       "                      [ 0.0115, -0.0023, -0.0041,  ..., -0.0013, -0.0029, -0.0019],\n",
       "                      [ 0.0007, -0.0016,  0.0130,  ...,  0.0094, -0.0063, -0.0009]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0118, -0.0090, -0.0089,  ..., -0.0056, -0.0100, -0.0016])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0252, -0.0211,  0.0151,  ...,  0.0205,  0.0430, -0.0071],\n",
       "                      [-0.0205,  0.0068,  0.0029,  ..., -0.0097,  0.0278,  0.0069],\n",
       "                      [-0.0273, -0.0071,  0.0171,  ..., -0.0232,  0.0262,  0.0302],\n",
       "                      ...,\n",
       "                      [-0.0243,  0.0234, -0.0263,  ...,  0.0131,  0.0110, -0.0119],\n",
       "                      [ 0.0145, -0.0287,  0.0216,  ..., -0.0117,  0.0290,  0.0031],\n",
       "                      [-0.0236,  0.0002,  0.0125,  ...,  0.0275,  0.0090, -0.0245]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0102, -0.0214,  0.0122, -0.0061,  0.0190,  0.0072, -0.0187, -0.0221,\n",
       "                      -0.0165,  0.0086,  0.0277, -0.0083,  0.0390, -0.0094,  0.0198, -0.0291,\n",
       "                      -0.0259,  0.0118,  0.0179,  0.0243, -0.0126,  0.0121,  0.0325,  0.0152,\n",
       "                       0.0132, -0.0063, -0.0148,  0.0027, -0.0012, -0.0238,  0.0281,  0.0136,\n",
       "                      -0.0051,  0.0177,  0.0222, -0.0233, -0.0085, -0.0234,  0.0187, -0.0137,\n",
       "                      -0.0231,  0.0012,  0.0121, -0.0233,  0.0339, -0.0192, -0.0212, -0.0221,\n",
       "                      -0.0345, -0.0111,  0.0065, -0.0175, -0.0002,  0.0234,  0.0013, -0.0105,\n",
       "                      -0.0153, -0.0192,  0.0092,  0.0242,  0.0300,  0.0241,  0.0238, -0.0179])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.0271,  0.0814, -0.1241,  ..., -0.0263, -0.1355,  0.0408],\n",
       "                      [ 0.0720,  0.0912,  0.0758,  ..., -0.1043,  0.0055,  0.1181],\n",
       "                      [-0.1449, -0.0630, -0.0564,  ...,  0.0121,  0.0683,  0.1199],\n",
       "                      ...,\n",
       "                      [-0.0368,  0.0607,  0.0607,  ...,  0.0080,  0.0913, -0.0574],\n",
       "                      [-0.0233, -0.0929,  0.0289,  ...,  0.0946,  0.0105,  0.1026],\n",
       "                      [-0.0527, -0.1654,  0.1053,  ...,  0.0395, -0.0151,  0.0688]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.0335, -0.1227,  0.0164, -0.0045, -0.0336, -0.0920, -0.0361,  0.0668,\n",
       "                       0.1082, -0.0199,  0.0418, -0.0852, -0.0969,  0.1425,  0.1140,  0.0197,\n",
       "                      -0.0091, -0.0760, -0.0216,  0.0605,  0.0390, -0.0475, -0.0024,  0.0891,\n",
       "                       0.1055, -0.0470,  0.0151,  0.0592,  0.0490, -0.0850, -0.0442,  0.0749])),\n",
       "             ('fc4.weight',\n",
       "              tensor([[ 0.5270, -0.0333, -0.2964, -0.0795, -0.3580, -0.0692,  0.0024,  0.2396,\n",
       "                       -0.3709, -0.0857,  0.4097,  0.0835, -0.1519,  0.2197,  0.1212, -0.0445,\n",
       "                       -0.2608,  0.0780,  0.0350, -0.3460,  0.4404, -0.0147, -0.3860, -0.0362,\n",
       "                       -0.3158,  0.0645, -0.2877, -0.4484, -0.3552, -0.0158, -0.2635, -0.3549]])),\n",
       "             ('fc4.bias', tensor([0.4028]))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = train_model(model=model,\n",
    "                n_epochs=10,\n",
    "                criterion=criterion_MSE, \n",
    "                optimizer=optimizer_sgd, \n",
    "                scheduler=sgd_lr_scheduler,\n",
    "                data_loader=data_loader,\n",
    "                prediction_threshold=0.5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87b29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300421de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <torch.utils.data.dataloader.DataLoader at 0x1efe29cb620>,\n",
       " 1: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea4350>,\n",
       " 2: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7e30>,\n",
       " 3: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7d70>,\n",
       " 4: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7cb0>,\n",
       " 5: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7bf0>,\n",
       " 6: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7b30>,\n",
       " 7: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea7a70>,\n",
       " 8: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea79b0>,\n",
       " 9: <torch.utils.data.dataloader.DataLoader at 0x1efe2ea78f0>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea99b26c348253",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(38373) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(38374) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(38375) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(38376) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "then tests the model on it.\n",
    "\n",
    "input: model: torch.nn.Module, the trained model\n",
    "       loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "compute: None, the function saves the predictions to a results.txt file\n",
    "\"\"\"\n",
    "model.eval()\n",
    "predictions = []\n",
    "# Iterate over the test data\n",
    "with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "    for [x_batch] in test_loader:\n",
    "        x_batch= x_batch.to(device)\n",
    "        predicted = model(x_batch)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        # Rounding the predictions to 0 or 1\n",
    "        predicted[predicted >= 0.5] = 1\n",
    "        predicted[predicted < 0.5] = 0\n",
    "        predictions.append(predicted)\n",
    "    predictions = np.vstack(predictions)\n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    progress_bar = tqdm(train_loader)\n",
    "    for data, target in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_description(f'Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
