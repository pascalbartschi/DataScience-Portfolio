{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine pytroch and sklearn -> skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "# data handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The device is automatically set to GPU if available, otherwise CPU\n",
    "# If you want to force the device to CPU, you can change the line to\n",
    "# device = torch.device(\"cpu\")\n",
    "# When using the GPU, it is important that your model and all data are on the \n",
    "# same device.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line.strip())\n",
    "\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                        transform=None)\n",
    "    filenames = [s[0].split('/')[-1].split(\"\\\\\")[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    # Normalize the embeddings\n",
    "    embeddings = (embeddings - embeddings.mean(axis = 1)[:, np.newaxis]) / embeddings.std(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTasteClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier with 4 fully connected layers, activated by ReLu and including dropout\n",
    "    for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, dropout_proba):\n",
    "        #inherit from Module\n",
    "        super().__init__()\n",
    "\n",
    "        # specify layer objects\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_proba)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_proba)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size3)\n",
    "        self.dropout3 = nn.Dropout(dropout_proba)\n",
    "        self.fc4 = nn.Linear(hidden_size3, 1)\n",
    "\n",
    "        # initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        # layer1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # layer2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # layer3\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "\n",
    "# load the training data\n",
    "X, y = get_data(TRAIN_TRIPLETS)\n",
    "\n",
    "# convert to tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# convert network to available with skorch\n",
    "# create model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    PimaClassifier,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.SGD,\n",
    "    max_epochs=100,\n",
    "    batch_size=10,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'optimizer__lr': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'optimizer__momentum': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'optimizer__lr': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'optimizer__momentum': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
